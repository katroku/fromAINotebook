{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credits to Abishek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "#for TPU\n",
    "import torch_xla.core.xla_model as xm\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self,bert_path):\n",
    "        super(BERTBaseUncased,self).__init__()\n",
    "        self.bert_path = bert_path\n",
    "        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768,30)\n",
    "        \n",
    "    #similar to transformers.BertModel.forward\n",
    "    def forward(self,ids,mask,token_type_ids):\n",
    "        #calls transformers.BertModel.forward\n",
    "        #transformers.BertModel.forward overrides __call__() method\n",
    "        _,o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        bo = self.bert_drop(o2)\n",
    "        return self.out(bo)\n",
    "\n",
    "class BERTDatasetTraining:\n",
    "    def __init__(self,qtitle,qbody,answer,targets,tokenizer,max_len):\n",
    "        self.qtitle = qtitle\n",
    "        self.qbody = qbody\n",
    "        self.answer= answer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.targets = targets #a numpy array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.answer)\n",
    "        #can be self.qbody etc.\n",
    "        \n",
    "    def __getitem__(self,item):\n",
    "        #in case \n",
    "        question_title = str(self.qtitle[item])\n",
    "        question_body = str(self.qbody[item])\n",
    "        answer = str(self.answer[item])\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question_title+\" \"+question_body,\n",
    "            answer,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        \n",
    "        ids = inputs[\"input_ids\"]\n",
    "        #ids of each token(words/subwords)\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        \n",
    "        #add padding\n",
    "        padding_len = self.max_len - len(ids)\n",
    "        ids = ids + ([0]*padding_len)\n",
    "        token_type_ids = token_type_ids + ([0]*padding_len)\n",
    "        mask = mask + ([0]*padding_len)\n",
    "        \n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids,dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask,dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.targets[item,:],dtype=torch.float)\n",
    "        }\n",
    "#custom loss function\n",
    "def loss_fn(outputs,targets):\n",
    "    return nn.BCEWithLogitsLoss()(outputs,targets)\n",
    "\n",
    "def train_loop_fn(data_loader,model,optimizer,device,scheduler=None):\n",
    "    model.train()\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        ids = d['ids']\n",
    "        mask = d['mask']\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        targets = d[\"targets\"]\n",
    "        \n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids=ids,mask=mask,token_type_ids=token_type_ids)\n",
    "        loss = loss_fn(outputs,targets)\n",
    "        loss.backward()\n",
    "        #for TPU,\n",
    "        xm.optimizer_step(optimizer, barrier=True) #barrier=True if using only 1 TPU node\n",
    "        #for GPU,\n",
    "        #optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if bi %10 == 0: #bi for batch index\n",
    "            print(f\"bi={bi}, loss={loss}\")\n",
    "            \n",
    "def eval_loop_fn(data_loader,model,device):\n",
    "    model.eval()\n",
    "    #there are 30 targets for google dataset\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    \n",
    "    for bi, d in enumerate(data_loader):\n",
    "        ids = d['ids']\n",
    "        mask = d['mask']\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        targets = d[\"targets\"]\n",
    "        \n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "        \n",
    "        outputs = model(ids=ids,mask=mask,token_type_ids=token_type_ids)\n",
    "        #outputs params, there should be 2 tensors\n",
    "        loss = loss_fn(outputs,targets)\n",
    "        \n",
    "        fin_targets.append(targets.cpu().detach().numpy())\n",
    "        fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "        \n",
    "        #np.vstack stacks array nicely along the first axis??\n",
    "    return np.vstack(fin_outputs), np.vstack(fin_targets)\n",
    "\n",
    "#last layer is linear layer, should apply sigmoid function but ok since use spearman\n",
    "def run(): #index is for multiprocessing\n",
    "    MAX_LEN = 512\n",
    "    TRAIN_BATCH_SIZE = 4\n",
    "    EPOCHS = 20\n",
    "    \n",
    "    dfx = pd.read_csv(\"input/google-quest-challenge/train.csv\").fillna(\"none\")\n",
    "    df_train, df_valid = model_selection.train_test_split(dfx,random_state=42,test_size=0.1)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_valid = df_valid.reset_index(drop=True)\n",
    "    \n",
    "    sample = pd.read_csv('input/google-quest-challenge/sample_submission.csv')\n",
    "    target_cols = list(sample.drop(\"qa_id\", axis=1).columns)\n",
    "    train_targets = df_train[target_cols].values #30 different metrics\n",
    "    valid_targets = df_valid[target_cols].values #30 different metrics\n",
    "    \n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(\"input/bert_base_uncased/\")\n",
    "    \n",
    "    train_dataset = BERTDatasetTraining(\n",
    "        qtitle=df_train.question_title.values,\n",
    "        qbody=df_train.question_body.values,\n",
    "        answer=df_train.answer.values,\n",
    "        targets=train_targets,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    valid_dataset = BERTDatasetTraining(\n",
    "        qtitle=df_valid.question_title.values,\n",
    "        qbody=df_valid.question_body.values,\n",
    "        answer=df_valid.answer.values,\n",
    "        targets=valid_targets,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    \n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=4,  #explicitly stated\n",
    "        shuffle=False   #True or false is ok\n",
    "    )\n",
    "    \n",
    "    device = xm.xla_device() #for TPUE #\"cuda\" for GPU\n",
    "    lr = 3e-5\n",
    "    num_train_steps = int(len(train_dataset)/TRAIN_BATCH_SIZE * EPOCHS)\n",
    "    model = BERTBaseUncased(\"input/bert_base_uncased\").to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loop_fn(train_data_loader,model,optimizer,device,scheduler)\n",
    "        o, t = eval_loop_fn(valid_data_loader,model,device)\n",
    "        \n",
    "        #t is fin_targets\n",
    "        #o os fin_output params?\n",
    "        spear = []\n",
    "        for jj in range(t.shape[1]):\n",
    "            p1 = list(t[:,jj])\n",
    "            p2 = list(o[:,jj])\n",
    "            coef,_ = np.nan_to_num(stats.spearmanr(p1,p2)) #spearman rank scipy\n",
    "            spear.append(coef)\n",
    "        spear = np.mean(spear)\n",
    "        print(f\"epoch = {epoch},spearman={spear}\")\n",
    "        #WHEN using TPU\n",
    "        xm.save(model.state_dict(),\"model.bin\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    run()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
